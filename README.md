# PosterO: Structuring Layout Trees to Enable Language Models in Generalized Content-Aware Layout Generation

This repository contains the Pytorch implementation for "[PosterO: Structuring Layout Trees to Enable Language Models in Generalized Content-Aware Layout Generation](https://arxiv.org/abs/2505.07843)", CVPR 2025.

<img src="/comparison.png" alt="Visualized results">
<p align="center">Comparison of layouts generated by different approaches.</p>

## How to Run

### Prerequisites
- Environment
```
Python 3.10.13
CUDA 12.2
```
- Main Modules
```
torch==2.4.0
torchvision==0.19.0
timm==0.9.7
opencv-python==4.8.1.78
pandas==2.2.2
Pillow==10.0.1
segmentation-models-pytorch==0.3.4
vllm==0.5.5
CairoSVG==2.7.1
numpy==1.26.2
```
- Other Modules: Please refer to ```requirements.txt```

### Content-Aware Layout Generation
1. Data Preparation
- Download the [PKU PosterLayout dataset](https://github.com/PKU-ICST-MIPL/PosterLayout-CVPR2023) and [CGL dataset](https://github.com/minzhouGithub/CGL-GAN) from their official websites. Only for convenience, we provide the [processed annotation files](https://drive.google.com/drive/folders/1GGh02Zv0sDjTai3FE0uNPntm-Asj8ioD?usp=sharing) with a uniform csv format. Please make sure to obtain the corresponding agreement before use.
- Follow the instructions of [RALF](https://github.com/CyberAgentAILab/RALF) to preprocess the images, including:
    - ```inpainting.py```: results should be put under ```input``` directory.
    - ```saliency_detection.py```: results should be put under ```saliency``` and ```saliency_sub``` directories.
- The file structure should be as follows:
```
â”œâ”€â”€ AbsolutePath/to/DatasetDirectory
â”‚   â”œâ”€â”€ all
â”‚   â”‚   â””â”€â”€ annotation
â”‚   â”‚       â”œâ”€â”€ test.csv
â”‚   â”‚       â””â”€â”€ train.csv
â”‚   â”œâ”€â”€ pku
â”‚   â”‚   â”œâ”€â”€ annotation
â”‚   â”‚   â”‚   â””â”€â”€ ... (as above)
â”‚   â”‚   â””â”€â”€ image
â”‚   â”‚       â”œâ”€â”€ test
â”‚   â”‚       â”‚   â”œâ”€â”€ input
â”‚   â”‚       â”‚   â”œâ”€â”€ saliency
â”‚   â”‚       â”‚   â””â”€â”€ saliency_sub
â”‚   â”‚       â””â”€â”€ train
â”‚   â”‚           â”œâ”€â”€ input
â”‚   â”‚           â”œâ”€â”€ original
â”‚   â”‚           â”œâ”€â”€ saliency
â”‚   â”‚           â””â”€â”€ saliency_sub
â”‚   â””â”€â”€ cgl
â”‚       â”œâ”€â”€ annotation
â”‚       â”‚   â””â”€â”€ ... (as above)
â”‚       â””â”€â”€ image
â”‚           â””â”€â”€ ... (as above)
â””â”€â”€ PosterO
```

2. Design Intent Detection
- Specify AbsolutePath/to/DatasetDirectory in ```init_path.sh``` and execute the following command:
```
source init_path.sh
```
- Download the [weights](https://drive.google.com/drive/folders/1CUv13fZvySk1AV-r-7jbBX0wRCyVFFQG?usp=sharing) of the **design intent detection model** or train it from scratch following the [instructions](design_intent_detect/README.md).
- The file structure should be as follows:
```
PosterO
â””â”€â”€ design_intent_detect
    â”œâ”€â”€ all_128_1e-06_none
    â”‚   â””â”€â”€ ckpt/design_intent_all_epoch25.pth
    â”œâ”€â”€ pku_128_1e-06_none
    â”‚   â””â”€â”€ ckpt/design_intent_pku_epoch100.pth
    â”œâ”€â”€ cgl_128_1e-06_none
    â”‚   â””â”€â”€ ckpt/design_intent_cgl_epoch35.pth
    â””â”€â”€ ...
```
- Execute the following commands to obtain the design intent detection results. Noted that GPU IDs should be specified in ```test.sh```
```
cd design_intent_detect
source test.sh <DATASET> <PATH_TO_WEIGHT>
```
For example, ```source test.sh pku pku_128_1e-06_none/ckpt/design_intent_pku_epoch100.pth``` for the PKU PosterLayout dataset.
- Noted that the detection results for ```all``` **must be obtained before running the evaluation script below**, as they are used for calculating the intent-aware metrics.

3. Layout Generation by In-context Learning
- Download the weights of [LLaMA 3.1-8B](https://huggingface.co/meta-llama/Llama-3.1-8B) or other Large Language Models (LLM).
- Execute the following commands to obtain the inference results:
```
cd .. # go back to the PosterO directory
source infer.sh <GPU_ID> <DATASET> <ABSOLUTE_PATH_TO_LLM_DIR> <EXPERIMENT_NAME>
```
For example, ```source infer.sh 0 pku /home/mymachine/Meta-Llama-3.1-8B EXP1``` for the PKU PosterLayout dataset with the Meta-Llama-3.1-8B model. The inference results will be saved at ```PosterO/Meta-Llama-3.1-8B/pku/*EXP1*.pt```.

4. Evaluation
- Execute the following command to evaluate the generated layouts:
```
source eval.sh <DATASET> <PATH_TO_INFERENCE_RESULT>
```

### Generalized Content-Aware Layout Generation
1. Data Preparation
- Download the [PStylish7 dataset](https://drive.google.com/file/d/1QzavL_nZEkw6jn6o2fl2HAmp2JKUw5_k/view?usp=sharing) and unzip it. For convenience, we provide all processed data along and thus no need to preprocess them again.
- The file structure should be as follows:
```
â””â”€â”€ AbsolutePath/to/DatasetDirectory
    â””â”€â”€ PStylish7
        â”œâ”€â”€ chinese-poem
        â”‚   â”œâ”€â”€ image
        â”‚   â”œâ”€â”€ predm_zs
        â”‚   â”‚   â”œâ”€â”€ features
        â”‚   â”‚   â”œâ”€â”€ design_intent_bbox_test.pt
        â”‚   â”‚   â””â”€â”€ design_intent_bbox_train.pt
        â”‚   â”œâ”€â”€ test.csv
        â”‚   â””â”€â”€ train.csv
        â”œâ”€â”€ food-menu
        â”‚   â””â”€â”€ ... (as above)
        â”œâ”€â”€ kine-animals
        â”‚   â””â”€â”€ ... (as above)
        â”œâ”€â”€ london-subway
        â”‚   â””â”€â”€ ... (as above)
        â”œâ”€â”€ motivational-quote
        â”‚   â””â”€â”€ ... (as above)
        â”œâ”€â”€ movie-poster
        â”‚   â””â”€â”€ ... (as above)
        â””â”€â”€ travel-vintage
            â””â”€â”€ ... (as above)
```

2. Layout Generation by In-context Learning
- Specify AbsolutePath/to/DatasetDirectory in ```init_path.sh``` and execute the following command:
```
source init_path.sh
```
- Download the weights of [LLaMA 3.1-8B](https://huggingface.co/meta-llama/Llama-3.1-8B) or other Large Language Models (LLM).
- Execute the following commands to obtain the inference results:
```
cd generalized_setting
source infer.sh <GPU_ID> <ABSOLUTE_PATH_TO_LLM_DIR> <EXPERIMENT_NAME>
```
For example, ```source infer.sh 0 /home/mymachine/Meta-Llama-3.1-8B EXP2``` for the PKU PosterLayout dataset with the Meta-Llama-3.1-8B model. The inference results will be saved at ```PosterO/generalized_setting/Meta-Llama-3.1-8B/<category_group>/*EXP2*.pt```.

3. Evaluation
- Execute the following command to evaluate the generated layouts:
```
source eval.sh <PATH_TO_INFERENCE_RESULT>
```
Here, the ```<category_group>``` field in ```<PATH_TO_INFERENCE_RESULT>``` **must be replaced with a ```{}``` wildcard**, which allows results from different categories to be evaluated in one command. For example, ```source eval.sh /home/mymachine/PosterO/generalized_setting/Meta-Llama-3.1-8B/{}/myEXP2name.pt```.

## Citation
If our work is helpful for your research, please cite our papers:
```
@inproceedings{Hsu-CVPR2025-postero,
  title={PosterO: Structuring Layout Trees to Enable Language Models in Generalized Content-Aware Layout Generation},
  author={Hsu, HsiaoYuan and Peng, Yuxin},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2025}
}
```
```
@inproceedings{Hsu-ICIG2023-densitylayout,
    title={Densitylayout: Density-conditioned Layout {GAN} for Visual-Textual Presentation Designs},
    author={Hsu, HsiaoYuan and He, Xiangteng and Peng, Yuxin},
    booktitle={Proceedings of the International Conference on Image and Graphics},
    pages={187--199},
    year={2023}
}
```

## Contact me
For any questions or further information, please feel free to reach me with email kslh99@outlook.comðŸ«¡
