# PosterO: Structuring Layout Trees to Enable Language Models in Generalized Content-Aware Layout Generation

This repository contains the Pytorch implementation for "PosterO: Structuring Layout Trees to Enable Language Models in Generalized Content-Aware Layout Generation", CVPR 2025.

## How to Run

### Prerequisites
- Environment
```
Python 3.10.13
CUDA 12.2
```
- Main Modules
```
torch==2.4.0
torchvision==0.19.0
timm==0.9.7
opencv-python==4.8.1.78
pandas==2.2.2
Pillow==10.0.1
segmentation-models-pytorch==0.3.4
vllm==0.5.5
CairoSVG==2.7.1
numpy==1.26.2
```
- Other Modules
Please refer to the ```requirements.txt``` file.

### Content-Aware Layout Generation
1. Data Preparation
- Download the [PKU PosterLayout dataset](https://github.com/PKU-ICST-MIPL/PosterLayout-CVPR2023) and [CGL dataset](https://github.com/minzhouGithub/CGL-GAN) from their official websites. Only for convenience, we provide the [processed annotation files](https://drive.google.com/drive/folders/1GGh02Zv0sDjTai3FE0uNPntm-Asj8ioD?usp=sharing) with a uniform csv format. Please make sure to obtain the corresponding agreement before use.
- Follow the instructions of [RALF](https://github.com/CyberAgentAILab/RALF) to preprocess the images, including
    - ```inpainting.py```: results should be put under ```input``` directory.
    - ```saliency_detection.py```: results should be put under ```saliency``` and ```saliency_sub``` directories.
- The file structure should be as follows
```
├── AbsolutePath/to/DatasetDirectory
│   ├── all
│   │   └── annotation
│   │       ├── test.csv
│   │       └── train.csv
│   ├── pku
│   │   ├── annotation
│   │   │   └── ... (as above)
│   │   └── image
│   │       ├── test
│   │       │   ├── input
│   │       │   ├── saliency
│   │       │   └── saliency_sub
│   │       └── train
│   │           ├── input
│   │           ├── original
│   │           ├── saliency
│   │           └── saliency_sub
│   └── cgl
│       ├── annotation
│       │   └── ... (as above)
│       └── image
│           └── ... (as above)
└── PosterO
```

2. Design Intent Detection
- Specify AbsolutePath/to/DatasetDirectory in the ```init_path.sh``` file and execute the following command.
```
source init_path.sh
```
- Download the [weights](https://drive.google.com/drive/folders/1CUv13fZvySk1AV-r-7jbBX0wRCyVFFQG?usp=sharing) of the **design intent detection model** or train it from scratch following ```design_intent_detect/README.md```.
- The file structure should be as follows
```
PosterO
└── design_intent_detect
    ├── all_128_1e-06_none
    │   └── ckpt/design_intent_all_epoch25.pth
    ├── pku_128_1e-06_none
    │   └── ckpt/design_intent_pku_epoch100.pth
    ├── cgl_128_1e-06_none
    │   └── ckpt/design_intent_cgl_epoch35.pth
    └── ...
```
- Execute the following commands to obtain the design intent detection results. Noted that GPU IDs should be specified in the ```test.sh```.
```
cd design_intent_detect
source test.sh <DATASET> <PATH_TO_WEIGHT>
```
For example, ```source test.sh pku pku_128_1e-06_none/ckpt/design_intent_pku_epoch100.pth``` for the PKU PosterLayout dataset.
- Noted that the detection results for ```all``` must be obtained before running the evaluation script below, as they are used for calculating the intent-aware metrics.

3. Layout Generation by In-context Learning
- Download the weights of [LLaMA 3.1-8B](https://huggingface.co/meta-llama/Llama-3.1-8B) or other Large Language Models (LLM).
- Execute the following commands to obtain the inference results.
```
cd .. # go back to the PosterO directory
source infer.sh <GPU_ID> <DATASET> <ABSOLUTE_PATH_TO_LLM_DIR> <EXPERIMENT_NAME>
```
For example, ```source infer.sh 0 pku /home/mymachine/Meta-Llama-3.1-8B EXP1``` for the PKU PosterLayout dataset with the Meta-Llama-3.1-8B model. The inference results will be saved at ```PosterO/Meta-Llama-3.1-8B/pku/*EXP1*.pt```.

4. Evaluation
- Execute the following command to evaluate the generated layouts.
```
source eval.sh <DATASET> <PATH_TO_INFERENCE_RESULT>
```

### Generalized Content-Aware Layout Generation
1. Data Preparation
- Download the [PStylish7 dataset](https://drive.google.com/file/d/1e1mKrDEmBzcT1cGL5GEejOWPU6nGkUZ9/view?usp=sharing) and unzip it. For convenience, we provide all processed data along and thus no need to preprocess them again.
- The file structure should be as follows
```
└── AbsolutePath/to/DatasetDirectory
    └── PStylish7
        ├── chinese-poem
        │   ├── predm_zs
        │   │   ├── features
        │   │   ├── design_intent_bbox_test.pt
        │   │   └── design_intent_bbox_train.pt
        │   ├── test.csv
        │   └── train.csv
        ├── food-menu
        │   └── ... (as above)
        ├── kine-animals
        │   └── ... (as above)
        ├── london-subway
        │   └── ... (as above)
        ├── motivational-quote
        │   └── ... (as above)
        ├── movie-poster
        │   └── ... (as above)
        └── travel-vintage
            └── ... (as above)
```

2. Layout Generation by In-context Learning
- Specify AbsolutePath/to/DatasetDirectory in the ```init_path.sh``` file and execute the following command.
```
source init_path.sh
```
- Download the weights of [LLaMA 3.1-8B](https://huggingface.co/meta-llama/Llama-3.1-8B) or other Large Language Models (LLM).
- Execute the following commands to obtain the inference results.
```
cd generalized_setting
source infer.sh <GPU_ID> <ABSOLUTE_PATH_TO_LLM_DIR> <EXPERIMENT_NAME>
```
For example, ```source infer.sh 0 /home/mymachine/Meta-Llama-3.1-8B EXP2``` for the PKU PosterLayout dataset with the Meta-Llama-3.1-8B model. The inference results will be saved at ```PosterO/generalized_setting/Meta-Llama-3.1-8B/<category_group>/*EXP2*.pt```.

3. Evaluation
- Execute the following command to evaluate the generated layouts.
```
source eval.sh <PATH_TO_INFERENCE_RESULT>
```
Here, the ```<category_group>``` field in ```<PATH_TO_INFERENCE_RESULT>``` **must be replaced with a ```{}``` wildcard**, which allows results from different categories to be evaluated in one command. For example, ```source eval.sh /home/mymachine/PosterO/generalized_setting/Meta-Llama-3.1-8B/{}/myEXP2name.pt```.

## Citation
If our work is helpful for your research, please cite our papers:
```
@inproceedings{Hsu-CVPR2025-postero,
  title={PosterO: Structuring Layout Trees to Enable Language Models in Generalized Content-Aware Layout Generation},
  author={Hsu, HsiaoYuan and Peng, Yuxin},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2025}
}
```
```
@inproceedings{Hsu-ICIG2023-densitylayout,
    title={Densitylayout: Density-conditioned layout gan for visual-textual presentation designs},
    author={Hsu, HsiaoYuan and He, Xiangteng and Peng, Yuxin},
    booktitle={Proceedings of the International Conference on Image and Graphics},
    pages={187--199},
    year={2023}
}
```